{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Binarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing  import LabelEncoder\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score,confusion_matrix\n",
    "\n",
    "# Miscellaneous\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('..\\Dataset\\processed.csv')\n",
    "\n",
    "# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n",
    "numeric_features = ['Age']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Add binning here\n",
    "    ('binarizer', Binarizer(threshold=35))\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = ['Gender', 'Country', 'state', 'self_employed', 'family_history',\n",
    "                       'work_interfere', 'no_employees', 'remote_work', 'tech_company', 'benefits',\n",
    "                       'care_options', 'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
    "                       'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
    "                       'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
    "                       'mental_vs_physical', 'obs_consequence']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Add PCA to the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Add PCA here\n",
    "    # ('pca', TruncatedSVD(n_components=20))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "data_transformed = pipeline.fit_transform(data.drop(columns=['treatment'],axis=1))\n",
    "le = LabelEncoder()\n",
    "label_transformed = le.fit_transform(data['treatment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/preprocessed.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-26 18:07:40,103] - INFO - Evaluating LogisticRegression...\n",
      "[2024-07-26 18:07:42,270] - INFO - Best score for LogisticRegression: 0.8400315208825846\n",
      "[2024-07-26 18:07:42,271] - INFO - Time elapsed for LogisticRegression: 2.17 seconds\n",
      "[2024-07-26 18:07:42,278] - INFO - Accuracy for LogisticRegression: 0.8126984126984127\n",
      "[2024-07-26 18:07:42,279] - INFO - Classification Report for LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80       158\n",
      "           1       0.78      0.87      0.82       157\n",
      "\n",
      "    accuracy                           0.81       315\n",
      "   macro avg       0.82      0.81      0.81       315\n",
      "weighted avg       0.82      0.81      0.81       315\n",
      "\n",
      "[2024-07-26 18:07:42,279] - INFO - AUC Score for LogisticRegression: 0.8128880109650891\n",
      "[2024-07-26 18:07:42,281] - INFO - Evaluating DecisionTreeClassifier...\n",
      "[2024-07-26 18:07:42,546] - INFO - Best score for DecisionTreeClassifier: 0.8357930879207475\n",
      "[2024-07-26 18:07:42,546] - INFO - Time elapsed for DecisionTreeClassifier: 0.26 seconds\n",
      "[2024-07-26 18:07:42,554] - INFO - Accuracy for DecisionTreeClassifier: 0.819047619047619\n",
      "[2024-07-26 18:07:42,554] - INFO - Classification Report for DecisionTreeClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.69      0.79       158\n",
      "           1       0.75      0.95      0.84       157\n",
      "\n",
      "    accuracy                           0.82       315\n",
      "   macro avg       0.84      0.82      0.82       315\n",
      "weighted avg       0.84      0.82      0.82       315\n",
      "\n",
      "[2024-07-26 18:07:42,555] - INFO - AUC Score for DecisionTreeClassifier: 0.8194590018543901\n",
      "[2024-07-26 18:07:42,557] - INFO - Evaluating KNeighborsClassifier...\n",
      "[2024-07-26 18:07:42,633] - INFO - Best score for KNeighborsClassifier: 0.7256219745581448\n",
      "[2024-07-26 18:07:42,634] - INFO - Time elapsed for KNeighborsClassifier: 0.08 seconds\n",
      "[2024-07-26 18:07:42,654] - INFO - Accuracy for KNeighborsClassifier: 0.7746031746031746\n",
      "[2024-07-26 18:07:42,655] - INFO - Classification Report for KNeighborsClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78       158\n",
      "           1       0.79      0.74      0.77       157\n",
      "\n",
      "    accuracy                           0.77       315\n",
      "   macro avg       0.78      0.77      0.77       315\n",
      "weighted avg       0.78      0.77      0.77       315\n",
      "\n",
      "[2024-07-26 18:07:42,656] - INFO - AUC Score for KNeighborsClassifier: 0.7744900427315972\n",
      "[2024-07-26 18:07:42,656] - INFO - Evaluating RandomForestClassifier...\n",
      "[2024-07-26 18:08:39,285] - INFO - Best score for RandomForestClassifier: 0.8357987166497803\n",
      "[2024-07-26 18:08:39,285] - INFO - Time elapsed for RandomForestClassifier: 56.63 seconds\n",
      "[2024-07-26 18:08:39,304] - INFO - Accuracy for RandomForestClassifier: 0.8253968253968254\n",
      "[2024-07-26 18:08:39,304] - INFO - Classification Report for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81       158\n",
      "           1       0.78      0.90      0.84       157\n",
      "\n",
      "    accuracy                           0.83       315\n",
      "   macro avg       0.83      0.83      0.82       315\n",
      "weighted avg       0.83      0.83      0.82       315\n",
      "\n",
      "[2024-07-26 18:08:39,305] - INFO - AUC Score for RandomForestClassifier: 0.8256268644682738\n",
      "[2024-07-26 18:08:39,307] - INFO - Evaluating AdaBoostClassifier...\n",
      "c:\\Users\\rajab\\Desktop\\Health-Tracker-Web-Application\\.conda\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "[2024-07-26 18:08:46,890] - INFO - Best score for AdaBoostClassifier: 0.8326184847461443\n",
      "[2024-07-26 18:08:46,891] - INFO - Time elapsed for AdaBoostClassifier: 7.58 seconds\n",
      "[2024-07-26 18:08:46,915] - INFO - Accuracy for AdaBoostClassifier: 0.8158730158730159\n",
      "[2024-07-26 18:08:46,916] - INFO - Classification Report for AdaBoostClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.79       158\n",
      "           1       0.75      0.94      0.84       157\n",
      "\n",
      "    accuracy                           0.82       315\n",
      "   macro avg       0.84      0.82      0.81       315\n",
      "weighted avg       0.84      0.82      0.81       315\n",
      "\n",
      "[2024-07-26 18:08:46,916] - INFO - AUC Score for AdaBoostClassifier: 0.816274288478594\n",
      "[2024-07-26 18:08:46,917] - INFO - Evaluating GradientBoostingClassifier...\n",
      "[2024-07-26 18:09:21,534] - INFO - Best score for GradientBoostingClassifier: 0.8315602836879433\n",
      "[2024-07-26 18:09:21,535] - INFO - Time elapsed for GradientBoostingClassifier: 34.62 seconds\n",
      "[2024-07-26 18:09:21,543] - INFO - Accuracy for GradientBoostingClassifier: 0.8095238095238095\n",
      "[2024-07-26 18:09:21,544] - INFO - Classification Report for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.72      0.79       158\n",
      "           1       0.76      0.90      0.82       157\n",
      "\n",
      "    accuracy                           0.81       315\n",
      "   macro avg       0.82      0.81      0.81       315\n",
      "weighted avg       0.82      0.81      0.81       315\n",
      "\n",
      "[2024-07-26 18:09:21,544] - INFO - AUC Score for GradientBoostingClassifier: 0.8098040796581473\n",
      "[2024-07-26 18:09:21,545] - INFO - Evaluating XGBClassifier...\n",
      "[2024-07-26 18:09:26,167] - INFO - Best score for XGBClassifier: 0.8421479229989869\n",
      "[2024-07-26 18:09:26,168] - INFO - Time elapsed for XGBClassifier: 4.62 seconds\n",
      "[2024-07-26 18:09:26,178] - INFO - Accuracy for XGBClassifier: 0.8126984126984127\n",
      "[2024-07-26 18:09:26,179] - INFO - Classification Report for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80       158\n",
      "           1       0.78      0.88      0.82       157\n",
      "\n",
      "    accuracy                           0.81       315\n",
      "   macro avg       0.82      0.81      0.81       315\n",
      "weighted avg       0.82      0.81      0.81       315\n",
      "\n",
      "[2024-07-26 18:09:26,179] - INFO - AUC Score for XGBClassifier: 0.81290816737886\n",
      "[2024-07-26 18:09:26,180] - INFO - Best model: RandomForestClassifier\n",
      "[2024-07-26 18:09:26,181] - INFO - Best score: 0.825397\n",
      "[2024-07-26 18:09:26,181] - INFO - Best report: {'accuracy': 0.8253968253968254, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.88      0.75      0.81       158\\n           1       0.78      0.90      0.84       157\\n\\n    accuracy                           0.83       315\\n   macro avg       0.83      0.83      0.82       315\\nweighted avg       0.83      0.83      0.82       315\\n', 'auc': 0.8256268644682738, 'confusion_matrix': array([[119,  39],\n",
      "       [ 16, 141]], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"model_training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_transformed, label_transformed, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the hyperparameter search space for each model\n",
    "hyperparameter_spaces = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": np.logspace(-5, 5, 10),\n",
    "        \"penalty\": [\"l2\"],  \n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 5],\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 500, 1000],\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 5],\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 500, 1000],\n",
    "        \"learning_rate\": [0.1, 0.5, 1.0],\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 500, 1000],\n",
    "        \"learning_rate\": [0.1, 0.5, 1.0],\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 500, 1000],\n",
    "        \"learning_rate\": [0.1, 0.5, 1.0],\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "best_model, best_score, best_report = None, 0, None\n",
    "for model_name, hyperparameter_space in hyperparameter_spaces.items():\n",
    "    logging.info(f\"Evaluating {model_name}...\")\n",
    "    model = eval(model_name)()\n",
    "    grid_search = GridSearchCV(model, hyperparameter_space, cv=5, scoring=\"accuracy\", n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    start_time = time()\n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        elapsed_time = time() - start_time\n",
    "        \n",
    "        best_score_for_model = grid_search.best_score_\n",
    "        logging.info(f\"Best score for {model_name}: {best_score_for_model}\")\n",
    "        logging.info(f\"Time elapsed for {model_name}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        classification_rep = classification_report(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "        logging.info(f\"Accuracy for {model_name}: {accuracy}\")\n",
    "        logging.info(f\"Classification Report for {model_name}:\\n{classification_rep}\")\n",
    "        logging.info(f\"AUC Score for {model_name}: {auc_score}\")\n",
    "\n",
    "        # If this model is better than the previous best_model, update best_model and best_report\n",
    "        if accuracy > best_score:\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_score = accuracy\n",
    "            best_report = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"classification_report\": classification_rep,\n",
    "                \"auc\": auc_score,\n",
    "                \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed_time = time() - start_time\n",
    "        logging.error(f\"Error with {model_name} after {elapsed_time:.2f} seconds: {e}\")\n",
    "\n",
    "# Print the test results for the best model\n",
    "logging.info(\"Best model: %s\", type(best_model).__name__)\n",
    "logging.info(\"Best score: %f\", best_score)\n",
    "logging.info(\"Best report: %s\", best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model to a pickle file\n",
    "with open('../models/model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessor and model from the pickle files\n",
    "with open('../models/preprocessed.pkl', 'rb') as file:\n",
    "    preprocessor = pickle.load(file)\n",
    "\n",
    "with open('../models/model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "def test_predict():\n",
    "    # Create a new data point\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Timestamp\": [\"2023-10-10 12:00:00\"],\n",
    "        \"Age\": [35],\n",
    "        \"Gender\": [\"Male\"],\n",
    "        \"Country\": [\"United States\"],\n",
    "        \"state\": [\"CA\"],\n",
    "        \"self_employed\": [False],\n",
    "        \"family_history\": [True],\n",
    "        \"treatment\": [\"Yes\"],\n",
    "        \"work_interfere\": [\"Sometimes\"],\n",
    "        \"no_employees\": [26-100],\n",
    "        \"remote_work\": [True],\n",
    "        \"tech_company\": [True],\n",
    "        \"benefits\": [True],\n",
    "        \"care_options\": [True],\n",
    "        \"wellness_program\": [True],\n",
    "        \"seek_help\": [True],\n",
    "        \"anonymity\": [True],\n",
    "        \"leave\": [True],\n",
    "        \"mental_health_consequence\": [True],\n",
    "        \"phys_health_consequence\": [True],\n",
    "        \"coworkers\": [\"Yes\"],\n",
    "        \"supervisor\": [\"Yes\"],\n",
    "        \"mental_health_interview\": [\"Yes\"],\n",
    "        \"phys_health_interview\": [\"Yes\"],\n",
    "        \"mental_vs_physical\": [\"No\"],\n",
    "        \"obs_consequence\": [\"No\"]\n",
    "    })\n",
    "\n",
    "    # Preprocess the new data\n",
    "    new_data_transformed = preprocessor.transform(new_data.drop(columns=['treatment'],axis=1))\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(new_data_transformed)[0]\n",
    "    print(prediction)\n",
    "\n",
    "    # Assert that the prediction is 1 (for treatment)\n",
    "    assert prediction == 1, f\"Expected 1, but got {prediction}\"\n",
    "\n",
    "\n",
    "test_predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
